00:11:16-00:11:30 Andy: 对，我上周刚和他们团队聊过。他们确实做得很好，特别是在处理中英文混用和专业术语方面。他们团队的王工说，他们训练了一个专门针对我们公司业务场景的模型，准确率比通用模型高很多。
00:11:31-00:12:00 Lily: 具体说说看，他们是怎么做到的？
00:12:01-00:13:15 Andy: 他们主要做了几个优化：一是收集了大量公司内部的会议录音，包括各种场景下的会议，比如产品评审、技术讨论、项目复盘等；二是建立了一个专业术语库，包含了我们常用的技术术语和产品术语；三是针对不同场景做了模型微调，比如技术讨论场景和产品讨论场景用不同的模型。
00:13:16-00:13:30 Tom: 这个思路很好，不过他们是怎么处理多人同时说话的情况的？
00:13:31-00:14:15 Andy: 他们用了一个叫 Speaker Diarization 的技术，可以识别不同说话人。不过目前还在优化阶段，特别是在处理重叠说话的情况。他们团队的李工说，正在尝试用 Transformer 架构来改进这个问题。
00:14:16-00:15:30 Tom: 我听说他们还在测试一个叫 VAD（Voice Activity Detection）的技术，可以更准确地检测语音活动。这个技术对降噪也很有帮助。
00:15:31-00:16:45 Andy: 这个技术很实用。
00:16:46-00:17:00 Tom: 本地部署的话，硬件要求高吗？
00:17:01-00:17:15 Andy: 如果用蒸馏后的小模型，一台普通的服务器就能跑起来。他们测试过，8 核 CPU，32GB 内存的机器就能支持 10 个并发转写。
00:17:16-00:18:30 Tom: 那我们可以先做个 POC，用云服务快速验证一下效果。如果效果不错，再考虑本地部署。对了，说到数据安全，同声传译团队是怎么处理的？
00:18:31-00:18:45 Lily: 这是个好问题。
00:18:46-00:19:30 Tom: 他们用了数据加密和访问控制，所有录音和转写结果都存储在加密的存储桶里。而且他们还在开发一个数据脱敏功能，可以自动识别和脱敏敏感信息。
00:19:31-00:20:45 Lily: 这个功能很实用。不过说到开发周期，你们觉得大概需要多长时间？我觉得可以先做个简单的版本，支持基本的语音转写功能。
00:20:46-00:22:00 Andy: 如果只做基础功能的话，我觉得两周应该够了。我们可以先用同声传译团队的基础设施，这样可以节省很多时间。不过我觉得在开始开发之前，我们应该先做个简单的用户测试，收集一些真实的会议录音。
00:22:01-00:22:15 Tom: 两周时间够吗？
00:22:16-00:23:00 Andy: 如果只做最基础的功能，应该没问题。我们可以先实现核心功能，其他功能后续迭代。
